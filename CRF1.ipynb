{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# --------------------------------------------------\n",
    "#\n",
    "# CRF1.ipynb\n",
    "#\n",
    "# (1) Token lemmatization using Zemberek TurkishMorphology  (DO NOT lemmatize tokens starting with uppercase letter)\n",
    "# (2) Token features:\n",
    "#     (a) token\n",
    "#\n",
    "# Written by cetinsamet -*- cetin.samet@metu.edu.tr\n",
    "# May, 2019\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from seqeval.metrics import classification_report\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite import CRF\n",
    "import jpype as jp\n",
    "import pickle\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZEMBEREK_PATH = 'bin/zemberek-full.jar'\n",
    "\n",
    "# Start the JVM\n",
    "jp.startJVM(jp.getDefaultJVMPath(), '-ea', '-Djava.class.path=%s' % (ZEMBEREK_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TurkishMorphology = jp.JClass('zemberek.morphology.TurkishMorphology')\n",
    "morphology        = TurkishMorphology.createWithDefaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filepath):\n",
    "\n",
    "    text     = []\n",
    "    sentence = []\n",
    "\n",
    "    with open(filepath, 'r') as infile:\n",
    "        for line in infile:\n",
    "            word, _, _, _ = line.strip().split('\\t')\n",
    "\n",
    "            if word == '<S>':\n",
    "                text.append(sentence)\n",
    "                sentence = []\n",
    "                continue\n",
    "\n",
    "            sentence.append(line.strip())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainText = readFile('data/train.txt')\n",
    "validText = readFile('data/valid.txt')\n",
    "testText  = readFile('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeature(word, word_index, sentence):\n",
    "\n",
    "    feature = {'word' : word}\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures = []\n",
    "trainLabels   = []\n",
    "\n",
    "for sentence in trainText:\n",
    "    SENT     = []\n",
    "    features = []\n",
    "    labels   = []\n",
    "    for token in sentence:\n",
    "        word, _, _, label = token.split('\\t')\n",
    "        \n",
    "        if word[0] == word[0].upper():\n",
    "            root = word\n",
    "        else:\n",
    "            results = morphology.analyze(word).analysisResults\n",
    "            root    = results[0].getLemmas()[0] if results else word\n",
    "        SENT.append(root)\n",
    "        labels.append(label)\n",
    "        \n",
    "    for i, word in enumerate(SENT):\n",
    "        features.append(getFeature(word, i, SENT))\n",
    "    \n",
    "    trainFeatures.append(features)\n",
    "    trainLabels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "validFeatures = []\n",
    "validLabels   = []\n",
    "\n",
    "for sentence in validText:\n",
    "    SENT     = []\n",
    "    features = []\n",
    "    labels   = []\n",
    "    for token in sentence:\n",
    "        word, _, _, label = token.split('\\t')\n",
    "        \n",
    "        if word[0] == word[0].upper():\n",
    "            root = word\n",
    "        else:\n",
    "            results = morphology.analyze(word).analysisResults\n",
    "            root    = results[0].getLemmas()[0] if results else word\n",
    "        SENT.append(root)\n",
    "        labels.append(label)\n",
    "        \n",
    "    for i, word in enumerate(SENT):\n",
    "        features.append(getFeature(word, i, SENT))\n",
    "    \n",
    "    validFeatures.append(features)\n",
    "    validLabels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFeatures = []\n",
    "testLabels   = []\n",
    "\n",
    "for sentence in testText:\n",
    "    SENT     = []\n",
    "    features = []\n",
    "    labels   = []\n",
    "    for token in sentence:\n",
    "        word, _, _, label = token.split('\\t')\n",
    "        \n",
    "        if word[0] == word[0].upper():\n",
    "            root = word\n",
    "        else:\n",
    "            results = morphology.analyze(word).analysisResults\n",
    "            root    = results[0].getLemmas()[0] if results else word\n",
    "        SENT.append(root)\n",
    "        labels.append(label)\n",
    "        \n",
    "    for i, word in enumerate(SENT):\n",
    "        features.append(getFeature(word, i, SENT))\n",
    "    \n",
    "    testFeatures.append(features)\n",
    "    testLabels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvalFeatures = trainFeatures + validFeatures\n",
    "trainvalLabels   = trainLabels + validLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "/Users/cetinsamet/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 16.7min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 61.1min finished\n",
      "loading training data to CRFsuite: 100%|██████████| 32171/32171 [00:01<00:00, 16860.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 34673\n",
      "Seconds required: 0.404\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.008804\n",
      "c2: 0.002801\n",
      "num_memories: 6\n",
      "max_iterations: 100\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=0.73  loss=840729.12 active=34648 feature_norm=1.00\n",
      "Iter 2   time=1.84  loss=477898.36 active=34654 feature_norm=5.94\n",
      "Iter 3   time=0.33  loss=397499.61 active=33526 feature_norm=4.97\n",
      "Iter 4   time=1.35  loss=304123.62 active=34388 feature_norm=3.51\n",
      "Iter 5   time=0.32  loss=300509.23 active=34637 feature_norm=3.99\n",
      "Iter 6   time=0.31  loss=289494.28 active=34636 feature_norm=4.17\n",
      "Iter 7   time=0.32  loss=287070.89 active=34608 feature_norm=6.96\n",
      "Iter 8   time=0.31  loss=258969.09 active=34648 feature_norm=6.18\n",
      "Iter 9   time=0.29  loss=254119.65 active=34664 feature_norm=6.79\n",
      "Iter 10  time=0.31  loss=243407.70 active=34641 feature_norm=8.28\n",
      "Iter 11  time=0.30  loss=232997.51 active=34624 feature_norm=12.52\n",
      "Iter 12  time=0.30  loss=226569.75 active=34656 feature_norm=12.67\n",
      "Iter 13  time=0.32  loss=223595.24 active=34667 feature_norm=13.25\n",
      "Iter 14  time=0.42  loss=220767.14 active=34659 feature_norm=15.43\n",
      "Iter 15  time=0.45  loss=218231.37 active=34670 feature_norm=15.44\n",
      "Iter 16  time=0.44  loss=216103.88 active=34666 feature_norm=16.16\n",
      "Iter 17  time=0.49  loss=211031.84 active=34668 feature_norm=17.91\n",
      "Iter 18  time=0.38  loss=197934.27 active=34659 feature_norm=24.29\n",
      "Iter 19  time=0.90  loss=194978.16 active=34662 feature_norm=29.10\n",
      "Iter 20  time=0.51  loss=184106.16 active=34670 feature_norm=34.23\n",
      "Iter 21  time=0.36  loss=172139.68 active=34667 feature_norm=44.20\n",
      "Iter 22  time=0.35  loss=167240.82 active=34664 feature_norm=58.55\n",
      "Iter 23  time=0.36  loss=159757.81 active=34662 feature_norm=55.16\n",
      "Iter 24  time=0.34  loss=154132.02 active=34670 feature_norm=58.49\n",
      "Iter 25  time=0.38  loss=151095.18 active=34672 feature_norm=60.52\n",
      "Iter 26  time=0.39  loss=143645.51 active=34655 feature_norm=76.67\n",
      "Iter 27  time=0.38  loss=140643.62 active=34665 feature_norm=88.82\n",
      "Iter 28  time=0.42  loss=130061.66 active=34669 feature_norm=90.48\n",
      "Iter 29  time=0.39  loss=124778.09 active=34669 feature_norm=98.64\n",
      "Iter 30  time=0.36  loss=113334.91 active=34660 feature_norm=123.38\n",
      "Iter 31  time=0.38  loss=100621.71 active=34656 feature_norm=152.06\n",
      "Iter 32  time=0.33  loss=92038.50 active=34656 feature_norm=174.97\n",
      "Iter 33  time=0.42  loss=79932.31 active=34654 feature_norm=211.17\n",
      "Iter 34  time=0.49  loss=70049.59 active=34654 feature_norm=247.63\n",
      "Iter 35  time=0.36  loss=59289.38 active=34660 feature_norm=297.80\n",
      "Iter 36  time=0.35  loss=50790.94 active=34645 feature_norm=342.14\n",
      "Iter 37  time=0.34  loss=50210.75 active=34610 feature_norm=411.09\n",
      "Iter 38  time=0.35  loss=40511.57 active=34644 feature_norm=404.27\n",
      "Iter 39  time=0.46  loss=38820.29 active=34662 feature_norm=414.28\n",
      "Iter 40  time=0.37  loss=32128.17 active=34480 feature_norm=493.02\n",
      "Iter 41  time=0.42  loss=29680.12 active=34620 feature_norm=546.00\n",
      "Iter 42  time=0.48  loss=26780.51 active=34641 feature_norm=554.65\n",
      "Iter 43  time=0.44  loss=23927.78 active=34649 feature_norm=580.07\n",
      "Iter 44  time=0.36  loss=21580.97 active=34618 feature_norm=616.92\n",
      "Iter 45  time=0.42  loss=18989.84 active=34554 feature_norm=659.67\n",
      "Iter 46  time=0.35  loss=16275.51 active=34625 feature_norm=704.84\n",
      "Iter 47  time=0.36  loss=16176.80 active=34610 feature_norm=738.55\n",
      "Iter 48  time=0.41  loss=14083.55 active=34645 feature_norm=747.27\n",
      "Iter 49  time=0.42  loss=13656.34 active=34657 feature_norm=755.59\n",
      "Iter 50  time=0.38  loss=13496.02 active=34635 feature_norm=786.68\n",
      "Iter 51  time=0.35  loss=12606.28 active=34663 feature_norm=789.16\n",
      "Iter 52  time=0.36  loss=12488.38 active=34660 feature_norm=789.12\n",
      "Iter 53  time=0.38  loss=12101.46 active=34599 feature_norm=788.58\n",
      "Iter 54  time=0.36  loss=12091.47 active=34604 feature_norm=792.66\n",
      "Iter 55  time=0.35  loss=11821.04 active=34623 feature_norm=786.57\n",
      "Iter 56  time=0.33  loss=11713.56 active=34643 feature_norm=790.76\n",
      "Iter 57  time=0.35  loss=11604.14 active=34635 feature_norm=794.28\n",
      "Iter 58  time=0.37  loss=11485.02 active=34531 feature_norm=804.24\n",
      "Iter 59  time=0.37  loss=11457.37 active=34562 feature_norm=808.66\n",
      "Iter 60  time=0.39  loss=11272.94 active=34602 feature_norm=808.27\n",
      "Iter 61  time=0.31  loss=11232.19 active=34604 feature_norm=808.75\n",
      "Iter 62  time=0.31  loss=11141.52 active=34583 feature_norm=810.23\n",
      "Iter 63  time=0.30  loss=11069.84 active=34486 feature_norm=814.48\n",
      "Iter 64  time=0.31  loss=10917.44 active=34555 feature_norm=812.36\n",
      "Iter 65  time=0.30  loss=10882.98 active=34576 feature_norm=812.64\n",
      "Iter 66  time=0.29  loss=10811.57 active=34565 feature_norm=813.47\n",
      "Iter 67  time=0.31  loss=10734.78 active=34564 feature_norm=814.53\n",
      "Iter 68  time=0.33  loss=10678.26 active=34529 feature_norm=816.69\n",
      "Iter 69  time=0.94  loss=10638.34 active=34551 feature_norm=818.28\n",
      "Iter 70  time=0.32  loss=10573.38 active=34567 feature_norm=818.70\n",
      "Iter 71  time=0.33  loss=10532.51 active=34563 feature_norm=819.45\n",
      "Iter 72  time=0.33  loss=10468.84 active=34553 feature_norm=820.87\n",
      "Iter 73  time=0.34  loss=10439.75 active=34535 feature_norm=822.03\n",
      "Iter 74  time=0.34  loss=10364.42 active=34546 feature_norm=823.24\n",
      "Iter 75  time=0.34  loss=10332.53 active=34545 feature_norm=823.68\n",
      "Iter 76  time=0.33  loss=10288.21 active=34508 feature_norm=824.98\n",
      "Iter 77  time=0.31  loss=10248.00 active=34538 feature_norm=825.21\n",
      "Iter 78  time=0.32  loss=10219.38 active=34542 feature_norm=825.58\n",
      "Iter 79  time=0.31  loss=10177.65 active=34542 feature_norm=826.27\n",
      "Iter 80  time=0.30  loss=10154.43 active=34542 feature_norm=826.84\n",
      "Iter 81  time=0.30  loss=10134.17 active=34538 feature_norm=827.07\n",
      "Iter 82  time=0.30  loss=10110.35 active=34542 feature_norm=827.61\n",
      "Iter 83  time=0.28  loss=10083.25 active=34533 feature_norm=828.20\n",
      "Iter 84  time=0.29  loss=10079.94 active=34515 feature_norm=828.80\n",
      "Iter 85  time=0.30  loss=10043.89 active=34531 feature_norm=829.21\n",
      "Iter 86  time=0.31  loss=10027.82 active=34524 feature_norm=829.62\n",
      "Iter 87  time=0.29  loss=10006.16 active=34522 feature_norm=830.34\n",
      "Iter 88  time=0.29  loss=9987.65  active=34517 feature_norm=830.76\n",
      "Iter 89  time=0.29  loss=9969.82  active=34514 feature_norm=831.40\n",
      "Iter 90  time=0.32  loss=9953.18  active=34507 feature_norm=831.97\n",
      "Iter 91  time=0.30  loss=9938.02  active=34508 feature_norm=832.58\n",
      "Iter 92  time=0.33  loss=9924.85  active=34502 feature_norm=833.28\n",
      "Iter 93  time=0.32  loss=9911.69  active=34501 feature_norm=833.90\n",
      "Iter 94  time=0.35  loss=9900.42  active=34499 feature_norm=834.50\n",
      "Iter 95  time=0.30  loss=9888.89  active=34492 feature_norm=834.99\n",
      "Iter 96  time=0.34  loss=9876.17  active=34492 feature_norm=835.69\n",
      "Iter 97  time=0.31  loss=9867.25  active=34489 feature_norm=836.36\n",
      "Iter 98  time=0.32  loss=9856.24  active=34492 feature_norm=836.98\n",
      "Iter 99  time=0.34  loss=9848.12  active=34487 feature_norm=837.51\n",
      "Iter 100 time=0.33  loss=9838.94  active=34484 feature_norm=838.01\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 39.049\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 34484 (34673)\n",
      "Number of active attributes: 30701 (30799)\n",
      "Number of active labels: 15 (15)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.232\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=None, c2=None,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error...ne,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=True),\n",
       "          fit_params=None, iid='warn', n_iter=50, n_jobs=-1,\n",
       "          param_distributions={'c1': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1b03e1b080>, 'c2': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1b020b76a0>},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn',\n",
       "          scoring=make_scorer(flat_f1_score, average=weighted), verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define fixed parameters and parameters to search\n",
    "crf = CRF(  algorithm='lbfgs',\n",
    "            max_iterations=100,\n",
    "            all_possible_transitions=True,\n",
    "            verbose=True)\n",
    "\n",
    "params_space = {'c1': scipy.stats.expon(scale=0.5),\n",
    "                'c2': scipy.stats.expon(scale=0.05)}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted')\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "\n",
    "rs.fit(trainvalFeatures, trainvalLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'c1': 0.008804228477113443, 'c2': 0.0028006600124066087}\n",
      "best CV score: 0.9522287185032302\n",
      "model size: 2.36M\n"
     ]
    }
   ],
   "source": [
    "# crf = rs.best_estimator_\n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.008804228477113443,   # <-- Found by applying grid search\n",
    "    c2=0.0028006600124066087,  # <-- Found by applying grid search\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|██████████| 32171/32171 [00:01<00:00, 21579.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 34673\n",
      "Seconds required: 0.303\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.008804\n",
      "c2: 0.002801\n",
      "num_memories: 6\n",
      "max_iterations: 100\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=0.61  loss=840729.12 active=34648 feature_norm=1.00\n",
      "Iter 2   time=2.20  loss=477898.36 active=34654 feature_norm=5.94\n",
      "Iter 3   time=0.47  loss=397499.61 active=33526 feature_norm=4.97\n",
      "Iter 4   time=1.15  loss=304123.62 active=34388 feature_norm=3.51\n",
      "Iter 5   time=0.35  loss=300509.23 active=34637 feature_norm=3.99\n",
      "Iter 6   time=0.53  loss=289494.28 active=34636 feature_norm=4.17\n",
      "Iter 7   time=0.50  loss=287070.89 active=34608 feature_norm=6.96\n",
      "Iter 8   time=0.38  loss=258969.09 active=34648 feature_norm=6.18\n",
      "Iter 9   time=0.28  loss=254119.65 active=34664 feature_norm=6.79\n",
      "Iter 10  time=0.29  loss=243407.70 active=34641 feature_norm=8.28\n",
      "Iter 11  time=0.31  loss=232997.51 active=34624 feature_norm=12.52\n",
      "Iter 12  time=0.27  loss=226569.75 active=34656 feature_norm=12.67\n",
      "Iter 13  time=0.28  loss=223595.24 active=34667 feature_norm=13.25\n",
      "Iter 14  time=0.30  loss=220767.14 active=34659 feature_norm=15.43\n",
      "Iter 15  time=0.30  loss=218231.37 active=34670 feature_norm=15.44\n",
      "Iter 16  time=0.28  loss=216103.88 active=34666 feature_norm=16.16\n",
      "Iter 17  time=0.30  loss=211031.84 active=34668 feature_norm=17.91\n",
      "Iter 18  time=0.31  loss=197934.27 active=34659 feature_norm=24.29\n",
      "Iter 19  time=0.59  loss=194978.16 active=34662 feature_norm=29.10\n",
      "Iter 20  time=0.38  loss=184106.16 active=34670 feature_norm=34.23\n",
      "Iter 21  time=0.33  loss=172139.68 active=34667 feature_norm=44.20\n",
      "Iter 22  time=0.29  loss=167240.82 active=34664 feature_norm=58.55\n",
      "Iter 23  time=0.30  loss=159757.81 active=34662 feature_norm=55.16\n",
      "Iter 24  time=0.31  loss=154132.02 active=34670 feature_norm=58.49\n",
      "Iter 25  time=0.30  loss=151095.18 active=34672 feature_norm=60.52\n",
      "Iter 26  time=0.29  loss=143645.51 active=34655 feature_norm=76.67\n",
      "Iter 27  time=0.60  loss=140643.62 active=34665 feature_norm=88.82\n",
      "Iter 28  time=0.67  loss=130061.66 active=34669 feature_norm=90.48\n",
      "Iter 29  time=0.43  loss=124778.09 active=34669 feature_norm=98.64\n",
      "Iter 30  time=0.33  loss=113334.91 active=34660 feature_norm=123.38\n",
      "Iter 31  time=0.31  loss=100621.71 active=34656 feature_norm=152.06\n",
      "Iter 32  time=0.27  loss=92038.50 active=34656 feature_norm=174.97\n",
      "Iter 33  time=0.28  loss=79932.31 active=34654 feature_norm=211.17\n",
      "Iter 34  time=0.57  loss=70049.59 active=34654 feature_norm=247.63\n",
      "Iter 35  time=0.30  loss=59289.38 active=34660 feature_norm=297.80\n",
      "Iter 36  time=0.54  loss=50790.94 active=34645 feature_norm=342.14\n",
      "Iter 37  time=0.55  loss=50210.75 active=34610 feature_norm=411.09\n",
      "Iter 38  time=0.36  loss=40511.57 active=34644 feature_norm=404.27\n",
      "Iter 39  time=0.32  loss=38820.29 active=34662 feature_norm=414.28\n",
      "Iter 40  time=0.28  loss=32128.17 active=34480 feature_norm=493.02\n",
      "Iter 41  time=0.44  loss=29680.12 active=34620 feature_norm=546.00\n",
      "Iter 42  time=0.32  loss=26780.51 active=34641 feature_norm=554.65\n",
      "Iter 43  time=0.30  loss=23927.78 active=34649 feature_norm=580.07\n",
      "Iter 44  time=0.30  loss=21580.97 active=34618 feature_norm=616.92\n",
      "Iter 45  time=0.31  loss=18989.84 active=34554 feature_norm=659.67\n",
      "Iter 46  time=0.28  loss=16275.51 active=34625 feature_norm=704.84\n",
      "Iter 47  time=0.32  loss=16176.80 active=34610 feature_norm=738.55\n",
      "Iter 48  time=0.30  loss=14083.55 active=34645 feature_norm=747.27\n",
      "Iter 49  time=0.29  loss=13656.34 active=34657 feature_norm=755.59\n",
      "Iter 50  time=0.29  loss=13496.02 active=34635 feature_norm=786.68\n",
      "Iter 51  time=0.33  loss=12606.28 active=34663 feature_norm=789.16\n",
      "Iter 52  time=0.34  loss=12488.38 active=34660 feature_norm=789.12\n",
      "Iter 53  time=0.44  loss=12101.46 active=34599 feature_norm=788.58\n",
      "Iter 54  time=0.32  loss=12091.47 active=34604 feature_norm=792.66\n",
      "Iter 55  time=0.33  loss=11821.04 active=34623 feature_norm=786.57\n",
      "Iter 56  time=0.39  loss=11713.56 active=34643 feature_norm=790.76\n",
      "Iter 57  time=0.31  loss=11604.14 active=34635 feature_norm=794.28\n",
      "Iter 58  time=0.34  loss=11485.02 active=34531 feature_norm=804.24\n",
      "Iter 59  time=0.32  loss=11457.37 active=34562 feature_norm=808.66\n",
      "Iter 60  time=0.32  loss=11272.94 active=34602 feature_norm=808.27\n",
      "Iter 61  time=0.35  loss=11232.19 active=34604 feature_norm=808.75\n",
      "Iter 62  time=0.28  loss=11141.52 active=34583 feature_norm=810.23\n",
      "Iter 63  time=0.30  loss=11069.84 active=34486 feature_norm=814.48\n",
      "Iter 64  time=0.30  loss=10917.44 active=34555 feature_norm=812.36\n",
      "Iter 65  time=0.29  loss=10882.98 active=34576 feature_norm=812.64\n",
      "Iter 66  time=0.29  loss=10811.57 active=34565 feature_norm=813.47\n",
      "Iter 67  time=0.29  loss=10734.78 active=34564 feature_norm=814.53\n",
      "Iter 68  time=0.31  loss=10678.26 active=34529 feature_norm=816.69\n",
      "Iter 69  time=0.86  loss=10638.34 active=34551 feature_norm=818.28\n",
      "Iter 70  time=0.31  loss=10573.38 active=34567 feature_norm=818.70\n",
      "Iter 71  time=0.47  loss=10532.51 active=34563 feature_norm=819.45\n",
      "Iter 72  time=0.32  loss=10468.84 active=34553 feature_norm=820.87\n",
      "Iter 73  time=0.32  loss=10439.75 active=34535 feature_norm=822.03\n",
      "Iter 74  time=0.33  loss=10364.42 active=34546 feature_norm=823.24\n",
      "Iter 75  time=0.63  loss=10332.53 active=34545 feature_norm=823.68\n",
      "Iter 76  time=0.38  loss=10288.21 active=34508 feature_norm=824.98\n",
      "Iter 77  time=0.28  loss=10248.00 active=34538 feature_norm=825.21\n",
      "Iter 78  time=0.29  loss=10219.38 active=34542 feature_norm=825.58\n",
      "Iter 79  time=0.28  loss=10177.65 active=34542 feature_norm=826.27\n",
      "Iter 80  time=0.28  loss=10154.43 active=34542 feature_norm=826.84\n",
      "Iter 81  time=0.28  loss=10134.17 active=34538 feature_norm=827.07\n",
      "Iter 82  time=0.28  loss=10110.35 active=34542 feature_norm=827.61\n",
      "Iter 83  time=0.29  loss=10083.25 active=34533 feature_norm=828.20\n",
      "Iter 84  time=0.29  loss=10079.94 active=34515 feature_norm=828.80\n",
      "Iter 85  time=0.38  loss=10043.89 active=34531 feature_norm=829.21\n",
      "Iter 86  time=0.30  loss=10027.82 active=34524 feature_norm=829.62\n",
      "Iter 87  time=0.32  loss=10006.16 active=34522 feature_norm=830.34\n",
      "Iter 88  time=0.28  loss=9987.65  active=34517 feature_norm=830.76\n",
      "Iter 89  time=0.27  loss=9969.82  active=34514 feature_norm=831.40\n",
      "Iter 90  time=0.27  loss=9953.18  active=34507 feature_norm=831.97\n",
      "Iter 91  time=0.29  loss=9938.02  active=34508 feature_norm=832.58\n",
      "Iter 92  time=0.27  loss=9924.85  active=34502 feature_norm=833.28\n",
      "Iter 93  time=0.29  loss=9911.69  active=34501 feature_norm=833.90\n",
      "Iter 94  time=0.40  loss=9900.42  active=34499 feature_norm=834.50\n",
      "Iter 95  time=0.30  loss=9888.89  active=34492 feature_norm=834.99\n",
      "Iter 96  time=0.46  loss=9876.17  active=34492 feature_norm=835.69\n",
      "Iter 97  time=0.38  loss=9867.25  active=34489 feature_norm=836.36\n",
      "Iter 98  time=0.27  loss=9856.24  active=34492 feature_norm=836.98\n",
      "Iter 99  time=0.30  loss=9848.12  active=34487 feature_norm=837.51\n",
      "Iter 100 time=0.27  loss=9838.94  active=34484 feature_norm=838.01\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 37.673\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 34484 (34673)\n",
      "Number of active attributes: 30701 (30799)\n",
      "Number of active labels: 15 (15)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.193\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None,\n",
       "  c1=0.008804228477113443, c2=0.0028006600124066087,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf.fit(trainvalFeatures, trainvalLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is saved.\n"
     ]
    }
   ],
   "source": [
    "# SAVE CONDITIONAL RANDOM FIELDS MODEL\n",
    "with open('model/crf1.pickle', 'wb') as outfile:\n",
    "    pickle.dump(crf, outfile, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"model is saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD CONDITIONAL RANDOM FIELDS MODEL\n",
    "with open('model/crf1.pickle', 'rb') as infile:\n",
    "    crf = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TRAIN CLASSIFICATION REPORT ###\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      PERSON       0.98      0.98      0.98     11079\n",
      "     PERCENT       0.98      1.00      0.99       521\n",
      "    LOCATION       0.98      0.99      0.98      7762\n",
      "ORGANIZATION       0.97      0.97      0.97      7073\n",
      "       MONEY       0.96      0.94      0.95       484\n",
      "        DATE       0.77      0.77      0.77      2553\n",
      "        TIME       0.99      0.99      0.99       156\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     29628\n",
      "   macro avg       0.96      0.96      0.96     29628\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainPredLabels = crf.predict(trainFeatures)\n",
    "\n",
    "print(\"### TRAIN CLASSIFICATION REPORT ###\\n\")\n",
    "print(classification_report(trainLabels, trainPredLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### VAL CLASSIFICATION REPORT ###\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      PERSON       0.99      0.98      0.98      3397\n",
      "    LOCATION       0.98      0.98      0.98      1647\n",
      "ORGANIZATION       0.98      0.97      0.97      1961\n",
      "        DATE       0.75      0.76      0.76       550\n",
      "     PERCENT       0.97      1.00      0.98        96\n",
      "       MONEY       1.00      0.97      0.99       110\n",
      "        TIME       1.00      1.00      1.00        19\n",
      "\n",
      "   micro avg       0.97      0.96      0.96      7780\n",
      "   macro avg       0.97      0.96      0.96      7780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validPredLabels = crf.predict(validFeatures)\n",
    "\n",
    "print(\"### VAL CLASSIFICATION REPORT ###\\n\")\n",
    "print(classification_report(validLabels, validPredLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TEST CLASSIFICATION REPORT ###\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      PERSON       0.91      0.70      0.79      1594\n",
      "    LOCATION       0.89      0.76      0.82      1091\n",
      "ORGANIZATION       0.82      0.72      0.77       862\n",
      "     PERCENT       0.86      0.91      0.88       107\n",
      "        DATE       0.66      0.65      0.66       364\n",
      "       MONEY       0.85      0.73      0.78       113\n",
      "        TIME       0.90      0.39      0.55        23\n",
      "\n",
      "   micro avg       0.86      0.72      0.78      4154\n",
      "   macro avg       0.86      0.72      0.78      4154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testPredLabels  = crf.predict(testFeatures)\n",
    "\n",
    "print(\"### TEST CLASSIFICATION REPORT ###\\n\")\n",
    "print(classification_report(testLabels, testPredLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutting down the JVM\n",
    "jp.shutdownJVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
