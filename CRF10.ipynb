{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# --------------------------------------------------\n",
    "#\n",
    "# CRF10.ipynb\n",
    "#\n",
    "# Token features:\n",
    "#     (a) token (surface form)\n",
    "#     (b) is_first           : is token at the beginning of the sentence?\n",
    "#     (c) is_last            : is token at the end of the sentence?\n",
    "#     (d) is_capitalized     : does token start with a capital letter? \n",
    "#     (e) is_all_capitalized : is all letters of the token capitalized?\n",
    "#     (f) is_capitals_inside : is there any capitalized letter inside the token?\n",
    "#     (g) is_numeric         : is token numeric?\n",
    "#     (h) is_numeric_inside  : is numeric characters inside the token?\n",
    "#     (i) is_alphanumeric    : is token alphanumeric?\n",
    "#     (j) prefix-1           : first letter of the token\n",
    "#     (k) suffix-1           : last letter of the token\n",
    "#     (l) prefix-2           : first two letters of the token\n",
    "#     (m) suffix-2           : last two letters of the token\n",
    "#     (n) prefix-3           : first three letters of the token\n",
    "#     (o) suffix-3           : last three letters of the token\n",
    "#     (p) prefix-4           : first four letters of the token\n",
    "#     (q) suffix-4           : last four letters of the token\n",
    "#     (r) next-token         : following token\n",
    "#     (s) prev-token         : preceding token\n",
    "#     (t) 2-next-token       : second following token\n",
    "#     (u) 2-prev-token       : second preceding token\n",
    "#     (v) pos                : part-of-speech tag\n",
    "#     (w) next-pos           : part-of-speech tag of following word\n",
    "#     (x) prev-pos           : part-of-speech tag of preceding word\n",
    "#\n",
    "# Written by cetinsamet -*- cetin.samet@metu.edu.tr\n",
    "# May, 2019\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from seqeval.metrics import classification_report\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite import CRF\n",
    "from tqdm import tqdm\n",
    "import jpype as jp\n",
    "import pickle\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZEMBEREK_PATH = 'bin/zemberek-full.jar'\n",
    "\n",
    "# Start the JVM\n",
    "jp.startJVM(jp.getDefaultJVMPath(), '-ea', '-Djava.class.path=%s' % (ZEMBEREK_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TurkishMorphology = jp.JClass('zemberek.morphology.TurkishMorphology')\n",
    "morphology        = TurkishMorphology.createWithDefaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filepath):\n",
    "\n",
    "    text     = []\n",
    "    sentence = []\n",
    "\n",
    "    with open(filepath, 'r') as infile:\n",
    "        for line in infile:\n",
    "            word, _, _, _ = line.strip().split('\\t')\n",
    "\n",
    "            if word == '<S>':\n",
    "                text.append(sentence)\n",
    "                sentence = []\n",
    "                continue\n",
    "\n",
    "            sentence.append(line.strip())\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainText = readFile('data/train.txt')\n",
    "validText = readFile('data/valid.txt')\n",
    "testText  = readFile('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeature(token, token_index, sentence, pos, next_pos, prev_pos):\n",
    "\n",
    "    feature = {'token'             : token,\n",
    "               'is_first'          : token_index == 0,\n",
    "               'is_last'           : token_index == len(sentence) - 1,\n",
    "               \n",
    "               'is_capitalized'    : token[0].upper() == token[0],\n",
    "               'is_all_capitalized': token.upper() == token,\n",
    "               'is_capitals_inside': token[1:].lower() != token[1:],\n",
    "               'is_numeric'        : token.isdigit(),\n",
    "               'is_numeric_inside' : any([c.isdigit() for c in token]),\n",
    "               'is_alphanumeric'   : token.isalnum(),\n",
    "               \n",
    "               'prefix-1'          : token[0],\n",
    "               'suffix-1'          : token[-1],\n",
    "               \n",
    "               'prefix-2'          : '' if len(token) < 2  else token[:2],\n",
    "               'suffix-2'          : '' if len(token) < 2  else token[-2:],\n",
    "               \n",
    "               'prefix-3'          : '' if len(token) < 3  else token[:3],\n",
    "               'suffix-3'          : '' if len(token) < 3  else token[-3:],\n",
    "               \n",
    "               'prefix-4'          : '' if len(token) < 4  else token[:4],\n",
    "               'suffix-4'          : '' if len(token) < 4  else token[-4:],\n",
    "               \n",
    "               'prev-token'        : '' if token_index == 0 else sentence[token_index - 1],\n",
    "               'next-token'        : '' if token_index == len(sentence) - 1 else sentence[token_index + 1],\n",
    "               \n",
    "               '2-prev-token'      : '' if token_index <= 1 else sentence[token_index - 2],\n",
    "               '2-next-token'      : '' if token_index >= len(sentence) - 2 else sentence[token_index + 2],\n",
    "               \n",
    "               'pos'               : pos, \n",
    "               'next-pos'          : next_pos, \n",
    "               'prev-pos'          : prev_pos \n",
    "              }\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25736/25736 [02:28<00:00, 173.53it/s]\n"
     ]
    }
   ],
   "source": [
    "trainFeatures = []\n",
    "trainLabels   = []\n",
    "\n",
    "for sentence_ in tqdm(trainText):\n",
    "\n",
    "    sentence = [token.split('\\t')[0] for token in sentence_]\n",
    "    labels   = [token.split('\\t')[-1] for token in sentence_]\n",
    "    features = []\n",
    "    \n",
    "    sentence_str = ' '.join(sentence)\n",
    "    analysis = morphology.analyzeAndDisambiguate(sentence_str).bestAnalysis()\n",
    "    \n",
    "    for i, word in enumerate(sentence):\n",
    "        \n",
    "        if len(sentence) == 1:\n",
    "            pos = analysis[i].getPos().shortForm\n",
    "            next_pos = ''\n",
    "            prev_pos = ''\n",
    "        elif i == 0:\n",
    "            pos = analysis[i].getPos().shortForm\n",
    "            next_pos = analysis[i + 1].getPos().shortForm\n",
    "            prev_pos = ''\n",
    "        elif i == len(sentence) - 1:\n",
    "            pos = analysis[i].getPos().shortForm\n",
    "            next_pos = ''\n",
    "            prev_pos = analysis[i - 1].getPos().shortForm\n",
    "        else:\n",
    "            pos = analysis[i].getPos().shortForm\n",
    "            next_pos = analysis[i + 1].getPos().shortForm\n",
    "            prev_pos = analysis[i - 1].getPos().shortForm\n",
    "            \n",
    "        features.append(getFeature(word, i, sentence, pos, next_pos, prev_pos))\n",
    "    \n",
    "    trainFeatures.append(features)\n",
    "    trainLabels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6435/6435 [00:47<00:00, 136.73it/s]\n"
     ]
    }
   ],
   "source": [
    "validFeatures = []\n",
    "validLabels   = []\n",
    "\n",
    "for sentence_ in tqdm(validText):\n",
    "    \n",
    "    sentence = [token.split('\\t')[0] for token in sentence_]\n",
    "    labels   = [token.split('\\t')[-1] for token in sentence_]\n",
    "    features = []\n",
    "    \n",
    "    sentence_str = ' '.join(sentence)\n",
    "    analysis = morphology.analyzeAndDisambiguate(sentence_str).bestAnalysis()\n",
    "    \n",
    "    for i, word in enumerate(sentence):\n",
    "        \n",
    "        if len(sentence) == 1:\n",
    "            pos = analysis[i].getPos().shortForm\n",
    "            next_pos = ''\n",
    "            prev_pos = ''\n",
    "        elif i == 0:\n",
    "            pos = analysis[i].getPos().shortForm\n",
    "            next_pos = analysis[i + 1].getPos().shortForm\n",
    "            prev_pos = ''\n",
    "        elif i == len(sentence) - 1:\n",
    "            pos = analysis[i].getPos().shortForm\n",
    "            next_pos = ''\n",
    "            prev_pos = analysis[i - 1].getPos().shortForm\n",
    "        else:\n",
    "            pos = analysis[i].getPos().shortForm\n",
    "            next_pos = analysis[i + 1].getPos().shortForm\n",
    "            prev_pos = analysis[i - 1].getPos().shortForm\n",
    "            \n",
    "        features.append(getFeature(word, i, sentence, pos, next_pos, prev_pos))\n",
    "    \n",
    "    validFeatures.append(features)\n",
    "    validLabels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3328/3328 [00:22<00:00, 224.92it/s]\n"
     ]
    }
   ],
   "source": [
    "testFeatures = []\n",
    "testLabels   = []\n",
    "\n",
    "for sentence_ in tqdm(testText):\n",
    "    \n",
    "    sentence = [token.split('\\t')[0] for token in sentence_]\n",
    "    labels   = [token.split('\\t')[-1] for token in sentence_]\n",
    "    features = []\n",
    "    \n",
    "    sentence_str = ' '.join(sentence)\n",
    "    analysis = morphology.analyzeAndDisambiguate(sentence_str).bestAnalysis()\n",
    "    \n",
    "    for i, word in enumerate(sentence):\n",
    "        \n",
    "        if len(sentence) == 1:\n",
    "            pos = analysis[i].getPos().shortForm\n",
    "            next_pos = ''\n",
    "            prev_pos = ''\n",
    "        elif i == 0:\n",
    "            pos = analysis[i].getPos().shortForm\n",
    "            next_pos = analysis[i + 1].getPos().shortForm\n",
    "            prev_pos = ''\n",
    "        elif i == len(sentence) - 1:\n",
    "            pos = analysis[i].getPos().shortForm\n",
    "            next_pos = ''\n",
    "            prev_pos = analysis[i - 1].getPos().shortForm\n",
    "        else:\n",
    "            pos = analysis[i].getPos().shortForm\n",
    "            next_pos = analysis[i + 1].getPos().shortForm\n",
    "            prev_pos = analysis[i - 1].getPos().shortForm\n",
    "            \n",
    "        features.append(getFeature(word, i, sentence, pos, next_pos, prev_pos))\n",
    "    \n",
    "    testFeatures.append(features)\n",
    "    testLabels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvalFeatures = trainFeatures + validFeatures\n",
    "trainvalLabels   = trainLabels   + validLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "/Users/cetinsamet/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 171.6min\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed: 385.0min finished\n",
      "loading training data to CRFsuite: 100%|██████████| 32171/32171 [00:21<00:00, 1463.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 451164\n",
      "Seconds required: 6.011\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.064274\n",
      "c2: 0.087533\n",
      "num_memories: 6\n",
      "max_iterations: 100\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=2.74  loss=598645.53 active=450342 feature_norm=1.00\n",
      "Iter 2   time=2.95  loss=361051.05 active=444516 feature_norm=3.31\n",
      "Iter 3   time=1.34  loss=310887.96 active=432249 feature_norm=2.96\n",
      "Iter 4   time=4.01  loss=248426.12 active=436227 feature_norm=2.49\n",
      "Iter 5   time=1.49  loss=229783.13 active=447056 feature_norm=2.73\n",
      "Iter 6   time=1.52  loss=176744.61 active=443234 feature_norm=4.70\n",
      "Iter 7   time=1.48  loss=156590.17 active=431349 feature_norm=5.77\n",
      "Iter 8   time=1.43  loss=144250.43 active=429582 feature_norm=6.28\n",
      "Iter 9   time=1.33  loss=132262.03 active=418153 feature_norm=7.52\n",
      "Iter 10  time=1.58  loss=112476.49 active=375601 feature_norm=9.80\n",
      "Iter 11  time=1.48  loss=104718.37 active=334488 feature_norm=12.23\n",
      "Iter 12  time=1.55  loss=95842.33 active=325928 feature_norm=12.66\n",
      "Iter 13  time=1.60  loss=89042.48 active=309938 feature_norm=13.61\n",
      "Iter 14  time=1.56  loss=76160.86 active=288727 feature_norm=16.21\n",
      "Iter 15  time=1.32  loss=66077.34 active=278267 feature_norm=19.31\n",
      "Iter 16  time=1.27  loss=60306.63 active=270443 feature_norm=22.19\n",
      "Iter 17  time=1.31  loss=55450.15 active=264184 feature_norm=24.22\n",
      "Iter 18  time=1.42  loss=51510.07 active=252757 feature_norm=27.62\n",
      "Iter 19  time=1.46  loss=46530.44 active=244017 feature_norm=32.00\n",
      "Iter 20  time=1.50  loss=41773.83 active=234594 feature_norm=37.30\n",
      "Iter 21  time=1.48  loss=37448.58 active=226333 feature_norm=43.24\n",
      "Iter 22  time=1.37  loss=33831.97 active=222810 feature_norm=48.87\n",
      "Iter 23  time=1.34  loss=30077.96 active=219146 feature_norm=56.09\n",
      "Iter 24  time=1.43  loss=26788.99 active=209192 feature_norm=67.19\n",
      "Iter 25  time=1.45  loss=24190.12 active=206749 feature_norm=71.02\n",
      "Iter 26  time=1.49  loss=21898.32 active=199505 feature_norm=77.50\n",
      "Iter 27  time=1.46  loss=19324.33 active=188558 feature_norm=86.52\n",
      "Iter 28  time=1.40  loss=17122.90 active=186939 feature_norm=95.74\n",
      "Iter 29  time=1.39  loss=15285.45 active=181354 feature_norm=106.45\n",
      "Iter 30  time=1.59  loss=13911.86 active=180505 feature_norm=113.23\n",
      "Iter 31  time=1.54  loss=12566.75 active=175262 feature_norm=121.76\n",
      "Iter 32  time=1.75  loss=11454.81 active=164071 feature_norm=130.84\n",
      "Iter 33  time=1.86  loss=10586.58 active=163469 feature_norm=135.30\n",
      "Iter 34  time=1.40  loss=9883.03  active=162157 feature_norm=139.19\n",
      "Iter 35  time=3.84  loss=9688.33  active=158131 feature_norm=140.23\n",
      "Iter 36  time=1.34  loss=9067.55  active=153240 feature_norm=143.13\n",
      "Iter 37  time=1.29  loss=8759.08  active=146228 feature_norm=144.82\n",
      "Iter 38  time=1.52  loss=8364.12  active=140038 feature_norm=147.40\n",
      "Iter 39  time=1.29  loss=8127.94  active=135492 feature_norm=148.68\n",
      "Iter 40  time=1.21  loss=7912.76  active=127644 feature_norm=150.62\n",
      "Iter 41  time=1.03  loss=7737.91  active=125227 feature_norm=152.55\n",
      "Iter 42  time=1.29  loss=7601.29  active=123922 feature_norm=154.02\n",
      "Iter 43  time=1.24  loss=7486.92  active=121678 feature_norm=155.70\n",
      "Iter 44  time=1.37  loss=7391.69  active=117780 feature_norm=156.75\n",
      "Iter 45  time=1.38  loss=7305.76  active=116085 feature_norm=157.96\n",
      "Iter 46  time=1.30  loss=7233.08  active=113552 feature_norm=158.69\n",
      "Iter 47  time=1.23  loss=7170.18  active=110829 feature_norm=159.38\n",
      "Iter 48  time=1.20  loss=7113.69  active=109235 feature_norm=159.74\n",
      "Iter 49  time=1.07  loss=7062.79  active=107229 feature_norm=160.25\n",
      "Iter 50  time=1.06  loss=7019.20  active=106091 feature_norm=160.38\n",
      "Iter 51  time=1.03  loss=6978.51  active=104598 feature_norm=160.66\n",
      "Iter 52  time=1.28  loss=6943.70  active=103428 feature_norm=160.63\n",
      "Iter 53  time=1.33  loss=6912.30  active=102481 feature_norm=160.76\n",
      "Iter 54  time=1.96  loss=6882.68  active=101257 feature_norm=160.68\n",
      "Iter 55  time=1.49  loss=6856.61  active=100209 feature_norm=160.77\n",
      "Iter 56  time=1.70  loss=6834.94  active=99178 feature_norm=160.68\n",
      "Iter 57  time=1.64  loss=6814.74  active=98074 feature_norm=160.77\n",
      "Iter 58  time=1.72  loss=6796.01  active=96978 feature_norm=160.68\n",
      "Iter 59  time=1.91  loss=6778.83  active=96056 feature_norm=160.71\n",
      "Iter 60  time=1.61  loss=6763.82  active=95018 feature_norm=160.58\n",
      "Iter 61  time=1.47  loss=6749.88  active=94141 feature_norm=160.61\n",
      "Iter 62  time=1.30  loss=6737.38  active=93367 feature_norm=160.46\n",
      "Iter 63  time=1.18  loss=6725.61  active=92719 feature_norm=160.46\n",
      "Iter 64  time=1.18  loss=6714.39  active=92035 feature_norm=160.31\n",
      "Iter 65  time=1.17  loss=6704.28  active=91501 feature_norm=160.29\n",
      "Iter 66  time=1.23  loss=6695.24  active=90950 feature_norm=160.12\n",
      "Iter 67  time=1.25  loss=6686.67  active=90696 feature_norm=160.07\n",
      "Iter 68  time=1.30  loss=6678.45  active=90252 feature_norm=159.89\n",
      "Iter 69  time=1.24  loss=6670.52  active=89855 feature_norm=159.84\n",
      "Iter 70  time=1.18  loss=6663.33  active=89456 feature_norm=159.66\n",
      "Iter 71  time=1.10  loss=6656.52  active=89257 feature_norm=159.59\n",
      "Iter 72  time=1.15  loss=6650.68  active=88861 feature_norm=159.42\n",
      "Iter 73  time=1.16  loss=6645.37  active=88683 feature_norm=159.37\n",
      "Iter 74  time=1.22  loss=6640.42  active=88441 feature_norm=159.24\n",
      "Iter 75  time=1.24  loss=6635.46  active=88230 feature_norm=159.18\n",
      "Iter 76  time=1.27  loss=6630.92  active=87904 feature_norm=159.08\n",
      "Iter 77  time=1.25  loss=6626.94  active=87648 feature_norm=159.02\n",
      "Iter 78  time=1.15  loss=6622.90  active=87474 feature_norm=158.92\n",
      "Iter 79  time=1.10  loss=6619.10  active=87410 feature_norm=158.89\n",
      "Iter 80  time=1.22  loss=6615.53  active=87245 feature_norm=158.83\n",
      "Iter 81  time=1.46  loss=6612.25  active=87089 feature_norm=158.79\n",
      "Iter 82  time=1.36  loss=6609.34  active=86893 feature_norm=158.71\n",
      "Iter 83  time=1.26  loss=6606.36  active=86700 feature_norm=158.69\n",
      "Iter 84  time=1.27  loss=6603.31  active=86545 feature_norm=158.62\n",
      "Iter 85  time=1.31  loss=6600.52  active=86525 feature_norm=158.62\n",
      "Iter 86  time=1.20  loss=6598.09  active=86388 feature_norm=158.56\n",
      "Iter 87  time=1.10  loss=6595.85  active=86268 feature_norm=158.56\n",
      "Iter 88  time=1.15  loss=6593.63  active=86070 feature_norm=158.50\n",
      "Iter 89  time=1.15  loss=6591.42  active=86028 feature_norm=158.51\n",
      "Iter 90  time=1.20  loss=6589.24  active=85906 feature_norm=158.47\n",
      "Iter 91  time=1.24  loss=6587.24  active=85844 feature_norm=158.49\n",
      "Iter 92  time=1.28  loss=6585.42  active=85660 feature_norm=158.47\n",
      "Iter 93  time=1.22  loss=6583.85  active=85600 feature_norm=158.49\n",
      "Iter 94  time=1.12  loss=6582.08  active=85484 feature_norm=158.48\n",
      "Iter 95  time=1.26  loss=6580.45  active=85424 feature_norm=158.51\n",
      "Iter 96  time=1.59  loss=6578.94  active=85376 feature_norm=158.50\n",
      "Iter 97  time=1.57  loss=6577.53  active=85303 feature_norm=158.54\n",
      "Iter 98  time=1.68  loss=6576.04  active=85193 feature_norm=158.54\n",
      "Iter 99  time=1.75  loss=6574.74  active=85147 feature_norm=158.57\n",
      "Iter 100 time=1.54  loss=6573.32  active=85070 feature_norm=158.57\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 144.731\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 85070 (451164)\n",
      "Number of active attributes: 55219 (370758)\n",
      "Number of active labels: 15 (15)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.533\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None, c1=None, c2=None,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error...ne,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=True),\n",
       "          fit_params=None, iid='warn', n_iter=30, n_jobs=-1,\n",
       "          param_distributions={'c1': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1b0b70c6a0>, 'c2': <scipy.stats._distn_infrastructure.rv_frozen object at 0x1a20841588>},\n",
       "          pre_dispatch='2*n_jobs', random_state=123, refit=True,\n",
       "          return_train_score='warn',\n",
       "          scoring=make_scorer(flat_f1_score, average=weighted), verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define fixed parameters and parameters to search\n",
    "crf = CRF(  algorithm='lbfgs',\n",
    "            max_iterations=100,\n",
    "            all_possible_transitions=True,\n",
    "            verbose=True)\n",
    "\n",
    "params_space = {'c1': scipy.stats.expon(scale=0.5),\n",
    "                'c2': scipy.stats.expon(scale=0.05)}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted')\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=30,\n",
    "                        random_state=123,\n",
    "                        scoring=f1_scorer)\n",
    "\n",
    "rs.fit(trainvalFeatures, trainvalLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'c1': 0.06427400999055287, 'c2': 0.08753302619907762}\n",
      "best CV score: 0.9727992908193509\n",
      "model size: 4.99M\n"
     ]
    }
   ],
   "source": [
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None,\n",
       "  all_possible_transitions=True, averaging=None, c=None,\n",
       "  c1=0.06427400999055287, c2=0.08753302619907762,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=100,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = rs.best_estimator_\n",
    "crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crf.fit(trainvalFeatures, trainvalLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is saved.\n"
     ]
    }
   ],
   "source": [
    "# SAVE CONDITIONAL RANDOM FIELDS MODEL\n",
    "with open('model/crf10.pickle', 'wb') as outfile:\n",
    "    pickle.dump(crf, outfile, pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"model is saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD CONDITIONAL RANDOM FIELDS MODEL\n",
    "with open('model/crf10.pickle', 'rb') as infile:\n",
    "    crf = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TRAINVAL CLASSIFICATION REPORT ###\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      PERSON       1.00      1.00      1.00     14476\n",
      "ORGANIZATION       1.00      0.99      0.99      9034\n",
      "    LOCATION       1.00      1.00      1.00      9409\n",
      "       MONEY       1.00      1.00      1.00       594\n",
      "        TIME       1.00      1.00      1.00       175\n",
      "        DATE       1.00      1.00      1.00      3103\n",
      "     PERCENT       1.00      1.00      1.00       617\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     37408\n",
      "   macro avg       1.00      1.00      1.00     37408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainvalPredLabels = crf.predict(trainvalFeatures)\n",
    "\n",
    "print(\"### TRAINVAL CLASSIFICATION REPORT ###\\n\")\n",
    "print(classification_report(trainvalLabels, trainvalPredLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TEST CLASSIFICATION REPORT ###\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      PERSON       0.91      0.86      0.89      1594\n",
      "ORGANIZATION       0.85      0.79      0.82       862\n",
      "     PERCENT       0.99      0.93      0.96       107\n",
      "    LOCATION       0.90      0.86      0.88      1091\n",
      "        DATE       0.91      0.88      0.90       364\n",
      "       MONEY       0.94      0.78      0.85       113\n",
      "        TIME       0.90      0.83      0.86        23\n",
      "\n",
      "   micro avg       0.90      0.85      0.87      4154\n",
      "   macro avg       0.90      0.85      0.87      4154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testPredLabels  = crf.predict(testFeatures)\n",
    "\n",
    "print(\"### TEST CLASSIFICATION REPORT ###\\n\")\n",
    "print(classification_report(testLabels, testPredLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutting down the JVM\n",
    "jp.shutdownJVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
